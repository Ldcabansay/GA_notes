{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fbe86a",
   "metadata": {},
   "source": [
    "# LP4 Max SAT Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1904360",
   "metadata": {},
   "source": [
    "## 457 Max SAT\n",
    "\n",
    "We've seen now, several times, assess viability problem. We input once again is the Boolean formula in conjunctive normal form with N variables and M clauses. And the output from the SAT problem is an assignment, a true false assignment, for the N variables, so that the formula evaluates true or we simply output no, if there's no such satisfying assignment. Now, as we know the SAT problem is NP-complete. Hence, we can't hope to find a polynomial time algorithm for the SAT problem. Now, this is a search problem. Let's look at the optimization version. It's the Max-SAT problem. Now, in the Max-SAT problem the input is the same. It's a boolean formula in conjunctive normal form. And once again will use N to denote the number of variables and M for the number of clauses. The differences in terms of the output. Even if there's no satisfying assignment, we're still going to output an assignment. And here, we're going to output an assignment which maximizes the number of satisfied clauses. In the optimization version of the SAT problem, we want to find an assignment which satisfies many clauses as possible. Now, this is still a hard problem. In particular the Max-SAT problem is NP-hard. It's no longer a search problem so it is no longer in the class NP, because we have no way of verifying that the number of clauses satisfied is maximum. By clearly, this Max-SAT problem is at least as hard as the SAT problem. It's straightforward to reduce SAT to Max-SAT and therefore Max-SAT is NP-hard. So once again, we can't hope to solve the Max-SAT problem in polynomial time. Instead we're going to aim to approximate the Max-SAT problem and to do that we're going to use linear programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0902f23",
   "metadata": {},
   "source": [
    "## 458 Approximate Max SAT\n",
    "\n",
    "Let's look in detail at what we mean by an approximation algorithm for the Max-SAT problem. Consider an input to the Max-SAT, a formula F where it has M clauses and let's let M* denote the maximum number of satisfied clauses. So if we look over all assignments to the variables in F, the maximum number of clauses satisfied by any of those assignments is M* and this is of course dependent on F so let's denote as M* of F. Now let's simplify the notation a little bit and let's drop this of F part and let's simply denote it as M*, which is the solution to the Max-SAT problem. Now clearly M* is at most M. If M* equals M, then that means formula F is satisfiable. Now, we're going to construct an algorithm which we will denote it as to capital A. It's going to take a formula F as input and it's going output L on this input F and to be precise actually, its going to output an assignment which satisfies L clauses of F. So, L is the number of clauses satisfied by these assignments outputted by A. Now how does L compare to the optimal solution M*? Where in the first instance, we're going to guarantee that L is at least M* over 2. Even though we don't know M*, we're going to guarantee that the output of our algorithm is at least within a factor of one half of the optimal number of satisfied clauses. And if this holds for every formula F, then this is a one half approximation algorithm. And then later in this lecture we're going to prove one half to three quarters and we're going to get to three quarters approximation algorithm for the Max-SAT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0848bf0",
   "metadata": {},
   "source": [
    "## 459 Outline\n",
    "\n",
    "And we're going to start by looking at a very simple randomized algorithm, and we'll show that this simple algorithm achieves a one-half approximation algorithm for Max-SAT. After that, we're going to look at an LP-based algorithm and we're going to prove that this new LP-based algorithm achieves a 1-1_over_E approximation factor, so to improve the previous algorithm by a little bit. Finally, we're going to look at an algorithm which is a combination of these two algorithms, and this combined algorithm is going to achieve a three-quarters approximation. So let's dive into the simple scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d78b1a",
   "metadata": {},
   "source": [
    "## 460 Simple Scheme\n",
    "\n",
    "Now let's consider some input f to the max stack problem. And once again, let's denote the variables as X_1 through X_n. So there are n variables. And there are m clauses which we'll denote as C_1 through C_M. Now we're going to do the simplest possible scheme for making a true false assignment for these n variables. We're not even going to look at the formula f. We're simply going to assign these variables randomly to true and false. For each variable X_i, independently of the other ones, we're going to set X_i to be true with probability half, and false with probably half. So we simply flip a fair coin and if heads comes up, we set X_i to be true and if tails comes up, we said X_i to be false. And we do that for all n variables. Now how does this random assignment perform? We want to look at the expected performance of this random assignment. How do we measure the performance of the algorithm? Well, we measure the performance by looking at the number of clauses satisfied. Therefore, let W denote the number of satisfied clauses by this random assignment. Now since the assignment is random, W is also a random variable. Now since W is a random variable, we want to look at its expected value which in some sense is the average value of W. In particular, the expectation is the average value of W weighted by its probabilities. Now what are the possible values for W? Well, the minimum number of clauses we can satisfy is of course zero and the maximum number is at most m. So let's sum over the possible value. Let L denote the number of satisfied clauses, and L is going to vary between 0 and m. And then we get this value L and we have to weight it by the probability that the random variable W is equal to L. This is simply the definition of the expectation of a random variable. Now the expectation in this form is quite difficult to analyze. Why? Because whether a particular clause is satisfied or not is related to whether another clause is satisfied or not. Because they might have variables in common. Now what we're going to do is we're going to break up this random variable, which is the total number of satisfied clauses, into a clause by clause quantity. And in this manner we're going to be able to analyze each clause in isolation, independent of what happens for the other clauses. And in that way it's going to be straightforward to analyze the expected performance of the algorithm. So let's look at how exactly we do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bc9341",
   "metadata": {},
   "source": [
    "## 461 Expectation\n",
    "\n",
    "Once again capital W denotes a number of satisfied clauses for a random assignment. Each variable is assigned true or false with probability one half. Now, we want to look at a simpler quantity. In particular, we want to look at one clause in isolation. So, consider the J's clause. C sub J, and let's make a new random variable W sub J. Now, we're just talking about one clause, so either that clause is satisfied or not. Therefore, this new random variable is going to take either value 1 or 0. It takes value 1 if the clause CJ is satisfied and it takes value 0 if it's not satisfied. Now, what happens if we sum this quantity over all clauses? So, we look at the sum for J going from 1 to M of W sub J, where we get a 1 for every clause which is satisfied and 0 if it's not satisfied. So, this gives us a total count of the number of satisfied clauses. Therefore, it's equal to capital W. Now recall, our original goal was to analyze the expected of performance of the algorithm. So we want to look at the expectation of capital W, expected number of satisfied clauses. Now, we can plug in this identity, so we can replace capital W by this sum. We have the expectation of the sum over J of W sub J and now recall the new rarety or expectation. So, we can take the sum outside. The expectation of two random variables X and Y. Expectation of X plus Y is the same as expectation of X plus the expectation of Y. So, now we have that the expectation of W equals the sum over J of the expectation of WJ. Now, we can analyze this quantity expectation of WJ. Notice, this just depends on the clause CJ. It doesn't have anything to do with the other clauses. So, now we can focus on this clause in isolation. We don't have to look at any of the other clauses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be7879",
   "metadata": {},
   "source": [
    "## 462 Analysis\n",
    "\n",
    "We're looking at this random variable W sub J, which takes value 1 or 0 depending on whether the clause Cj is satisfied or not by the random assignment. We want to analyze the expectation of W sub J. So we take the two possible values weighted by the probability of achieving that value. So, we have value 1 times the probability that Wj equals 1 plus 0 times the probability Wj equals zero. That clearly, this term goes away and we're left with the probability Wj equals 1. This is the beauty of considering a random variable which takes value 0 or 1. The expectation is simply the probability that that random variable equals value 1. Now, considering a sample of clause Cj, suppose this x1, or x2 bar, or x3 bar, or x4 and so on up to xK. So the clause has size K. Now, in order to achieve value 1, the clause has to be satisfied. Let's look at the compliment. What's the probability the clause is not satisfied? Now, in order to not satisfy this clause, we have to set x1 to false, x2 to true, x3 to true and so on. That's exactly one setting of these variables, these K variables, so that this clause is not satisfied. The chance x1 is set to false is one-half. The chance that x1 is set to false and x2 is set to true and x3 is set to true and so on, so these K variables are set exactly to this one assignment. The probability of that one assignment is 2 to the minus K. That's the probability that this clause is not satisfied. What's the probability that is satisfied is a compliment of that, so it's 1 minus that. The probability that Wj equals 1 is equal to 1 minus 2 to the minus K. Now, notice something interesting, this gets better as K grows. The worst case is when K equals 1, it's a unit clause. So, suppose it's just x1, then what's the probability this clause does not satisfy is one-half and the probability it is satisfied is one-half. So, this quantity is always at least one-half. Since K is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f0789",
   "metadata": {},
   "source": [
    "## 463 Finishing Off\n",
    "\n",
    "Now, once again, we wanted to look at the expected number of satisfied clauses for this random assignment. We let W denote the random variable for the number of satisfied clauses, and we let WJ denote whether clause CJ is satisfied or not. And then this simplified to a sum over J from 1 to M, of whether the expectation of whether clause CJ is satisfied or not. And then we just prove that this expectation, the probability that clause CJ is satisfied, is at least 1/2. This is at least 1/2 for each of these M clauses. Therefore, this whole sum is at least M over 2. So what we've shown is a randomized algorithm, which achieves a one half approximation factor, though it's simply the expected performance of this algorithm. Now, it turns out we can de-randomize this algorithm. So we can find it's terministic algorithm which is guaranteed to find a 1/2 approximation, so the number of satisfied causes. And this uses the method of conditional expectations. It's quite simple, so let me quickly give you the idea of how it works. So we're going to run through the variables one by one, so let's let I go from 1 to N. Now we're going to try the two possible settings for XI, True or false, and we're going to compute the expected performance for each. So given this setting for XI, and given a fixed setting for X1 trough XI-1. So the early ones are fixed, and now we're trying both possible settings for XI. We compute the expected performance for a random assignment for the remaining variables. XI+1 to XN. Now this expected performance is quite easy to compute based on the analysis approach we just did. And what we're going to do is we're going to take the better of these two assignments, and then we're going to fix it, and then we're going to move on to the next variable. Now one interesting feature is that we're not saying something about the optimal number of satisfied clauses. For instance, our formula F might have 12 clauses, and maybe the maximum number of clauses that can be satisfied simultaneously is maybe 10. So M* in this instance would be 10, and M would be 12. Now, what we're proving is that a random assignment satisfies at least six of the clauses. Now since sets within a factor of 2 of the total number of clauses, therefore we're within half approximation of the maximum number of satisfied clauses. And actually, what was shown is that every formula has an assignment which satisfies at least half of the clauses. Intuitively, if the average is at least M/2, there must be at least one setting which achieves the average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e33cce4",
   "metadata": {},
   "source": [
    "## 464 Ek SAT\n",
    "\n",
    "There's one important point that we have to make about the performance of the algorithm which will be useful later. Instead of max-SAT, let's consider max-Ek-SAT. So every clause has size exactly K. For instance, let's consider K=3, and suppose every clause in our input formula f had size exactly 3. Now in this case what's the probability that a particular clause C_j is satisfied? Well, there's one setting of the three literals which appear in this clause so that this clause is not satisfied. The probability of that assignment is one-eighth. Therefore, the probability that is satisfied is exactly seven-eighths, and therefore for the special case of max-E3-SAT, we achieve a seven-eighths approximation algorithm. And what if instead of size 3, every clause had size exactly K, then the probability that a specific clause is not satisfied is two to the minus K. So the probability is satisfied is one minus SAT, and therefore we achieve a one minus two minus approximation algorithm for max-Ek-SAT. We're going to use later that this algorithm works well when all the clauses are large. And now we're going to make a LP based algorithm which works well when the clauses are small. Now, one interesting tidbit this seven-eighths approximation algorithm for max-E3-SAT is the best possible. Hovstad proved that it's NP hard to do any better than seven-eighths for this case. If we achieve an algorithm which has guaranteed performance seven-eighths plus epsilon for any epsilon. For instance, if we achieve 0.88 approximation algorithm, then that implies that P=NP. So this very simple naive algorithm is the best possible when all the causes are of size exactly 3. Thus the hard case is when the formula has varying size clauses, has some unit clauses, some clause of size two and some clauses of size three and so on. But if all the clauses are of the same size and they happen to be of size three, then we can achieve the best possible algorithm by just a random assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f36140",
   "metadata": {},
   "source": [
    "## 465 Integer Programming\n",
    "\n",
    "Our second approach, we'll use Linear programming to get an approximation algorithm for Max-SAT. To do that, we'll use a stronger form of Linear programming known as Integer Linear Programming. Before we get into that, first, let's recall the general form of Linear Programming. Is a linear programming canonical form? For the variable vector X, where maximizing C transpose X, subject to the following constraints. Ax â‰¤ b, and X is non-negative. This is a canonical form for Linear program. Now, in an Integer Linear Program, so in ILP, it has the same canonical form with one additional constraint, X is constrained to be in Z to the N, where Z are the integers. So if you think of the vector X of size N, this constraint is saying that each Xi is integer value. So what's going on geometrically? In the linear program, we had a feasible region, this convex set, define by these constraints, and we're trying to find the best real numbered point X in that feasible set, so that we maximize this objective function, but we consider all real numbered points in that set X. Now we're placing this grid, this N dimensional grid, and we're only looking at the grid points contained in that feasible set. Those are the only feasible points now. We want to find the best grid point which maximizes this objective function. Linear program had a nice property that there always is a vertex of the feasible region which is an optimal point, which maximizes this objective function. We no longer have such a property for Integer Linear Programming. And whereas linear programming is, polynomial time solvable, so it lies in the class P. In contrast, integer linear programming is NP-hard, and we're going to see that right now. We're going to see how to reduce Max-SAT to Integer Linear Programming. In fact, many of the NP complete problems that we've seen so far, such as vertex cover, independent set, and so on are easy to reduce to Integer Linear Programming. So it's quite powerful technique. Well since, ILP, Integer Lienear Programming is NP-hard, we can expect to solve it in polynomial time. So what's our game plan? Well first, we're going to see how to reduce SAT or Max-SAT to Integer Linear Programming, then we're going to look at the Linear Programming relaxation. So we're going to ignore this one constraint, this integer value constraint. So we're going to look at the best real number point X. Of course, the objective function might go up. We're going to use this real number point X which is the optimal solution to the Linear Program, to find an integer point which is nearby. That's going to give us a feasible solution to the Integer Linear Program, and then we'll see how far away it is from the optimal solution. And that will give us our approximation algorithm to the Max-SAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bce509",
   "metadata": {},
   "source": [
    "## 466 NP Hard\n",
    "\n",
    "We're going to prove now, that integer linear programming is NP-hard. To do that we're going to reduce the Max-SAT problem to integer linear program. Consider an input f for the Max-SAT problem and say that f has N variables and M constraints. In the integer linear program that we create, what are the variables going to be? Well, for each variable in the formula f, we can add a variable Y_i to the integer linear program that we create. And for each clause C_j, we're going to add a variable Z_j. So our initial linear program is going to have N + M variables and we're going to add the constraints that the Y_i's are between zero and one, and also each Z_j is between zero and one. And since these variables are constrained to be integer value, they'll either receive value one or zero. It can't receive any fractional value. Now intuitively what's going on? These Y_is are going to correspond to whether this variable X_i is set to True or false. So, Y_i equals one, corresponds to X_i being true. X_i being false corresponds to Y_i being zero. Z_j is going to correspond to whether this clause is satisfied or not. So it's going to take value one, if this clause is satisfied by the literals appearing in it and it's gonna take value zero, if this clause is not satisfied. That's the intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cc66db",
   "metadata": {},
   "source": [
    "## 467 Clauses\n",
    "\n",
    "We're going to have a constraint for every clause. Let's take an example clause to get some idea of what we want to achieve. Consider this clause X5, or X3, or X6. We made all the literals positive in order to make it a simple example. Now, what do we want from our constraint? What we want that if these variables are set to false. Now, setting X5 to false is going to correspond to setting Y5 to 0, and Y3 to 0. And finally, Y6 to 0. So if these three variables are set to 0, which are going to correspond to setting these variables to false, then this clause is not satisfied, then we want the variable for this clause to be value 0. So when this clause, this J clause is not satisfied, we want these variables ZJ to be constrained to be value 0. What if it's another setting? Well, then this clause is satisfied. In this case, ZJ will be value 0 or 1. We can't force it to be value 1, but what we can do is we can try to maximize these ZJ's, that'll be our objective function, and then the optimal point will take value 1. So if it can achieve value 1, then we will. In summary, the constraint we need to add is that if all three of these variables are 0, then ZJ is 0. How do we achieve that? We say that ZJ is at most Y5+Y3+Y6. So if these three variables are 0, then ZJ must be 0. Now if at least one of these is 1, then ZJ can takes values 0 or 1. Recall that these variables are constrained between 0 and 1, and also these Y's are constrained to be 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fa503",
   "metadata": {},
   "source": [
    "## 468 Another Clause\n",
    "\n",
    "Take a look at another clause that has some positive and negative literals. Consider this clause of size four: X1 bar, or X3, or X2, or X5 bar. Now, we want, that if X1 is set to true, X3 is false, X2 is false, and X5 is true which is going to correspond to Y1 being one, Y3 and Y2 being value zero and Y5 being value one. Then, we want that the variable Z, for this clause, let's say it's clause J, is forced to take value zero which corresponds to this clause being unsatisfied. Well, previously when we had just had positive form of these literals, then we looked at the sum of the Y's appearing. Now, we're going to look at the complement for these negative literals. So, we're going to look at one minus Y1 plus Y3 plus Y2 because these appear in positive form. Plus one minus Y5, because it appears as a negative form, and we're going to constrain ZJ to be at most this quantity. So, if this setting is true, then ZJ must be zero. For any other setting of these four Y's, then ZJ can be zero or one. And in general, for clause CJ, we have to consider which literals are positive form and which literals are in negative form. So, let's use CJ plus and CJ superscript minus. CJ plus will of course be the positive literals in the Jth clause, in this case, it's X2 and X3 and CJ minus will be the negative literals, in this case X1 and X5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aabc91",
   "metadata": {},
   "source": [
    "## 469 Reduction\n",
    "\n",
    "Now, we can look at general reduction. Consider Max-SAT input little f. Let's define the ILP. Our goal is to maximize the number of satisfied clauses, so we have a maximization problem. For each clause we have a variable Zj, which denotes whether the clause is satisfied or not. It's going to take value 1 if the clause is satisfied, and 0 if it's unsatisfied. So since we want to maximize the number of satisfied clauses, that means we want to maximize the number of clauses where the variables Zj equals 1. This sum, will be the number of satisfied clauses. Now what are the constraints? Recall we have a variable for each variable in the formula. And we also have a variable in the ILP for each clause in the formula. So, let's go over the variables in the formula first, so for i going from 1 to n, we're going to constraint Yi to be between 0 and 1. And since this is integer value, it takes value 0 or 1. Now, for each clause, similar constraint Zj takes value 0 or 1, and then we have the constraint for the clause that we just reviewed. So for Clause Zj, we look at the positive literals. So for each variable which appears in this clause in the positive form, the bad case is when that corresponding variable Yi is set to 0. That means that literal is not satisfied. Similarly for each variable which appears in the negative literal it's unsatisfied, when Yi takes value 1, which means 1 minus Yi has value 0. And we're going to impose this as an upper bound on Zj. So if all the literals in this clause are unsatisfied then Zj is forced to take value 0, so this clause is unsatisfied. And if at least one literal is satisfied and then Zj can takes value 0 or 1. And since we're maximizing, Zj will take value 1 if it can. Finally, we have the constraint that these variables, the Yi's and the Zj's are integer value. So they lie on this N dimensional grid. Actually in this case, we're in N plus M dimensions. And that defines our reduction from Max-SAT to integer linear programming. In fact it's equivalent, so this integer linear program is equivalent to the original Max-SAT problem on this in-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d9214",
   "metadata": {},
   "source": [
    "## 470 LP Relaxation\n",
    "\n",
    "Take this ILP, this interlinear program that we just defined, and consider an optimal point for this ILP. Of course we don't know how to find this optimal point in polynomial time, and we're not in fact finding it. We're just considering this optimal point for the purposes of the proof. So let Y-star and Z-star denote the optimal point for this ILP. Now what's the value of the objective function at the optimal point? Where the value of the objective function is simply the sum of the Z-stars. What do we know about the value of the objective function at this optimal point? Well it equals M-star, what is M-star? M-star is the maximum number of satisfied clauses in the original formula F. M-star is what we're trying to find and we can't solve this ILP in polynomial time, but we can solve any linear program in polynomial time. Can we convert this ILP into a similar LP? While this last constraint is what makes this an ILP so let's drop this last constraint. So, now the feasible points are no longer constrained to be integer value and then this changes from an ILP to a linear program this is simply a linear program now. We can solve this linear program in polynomial time, and let's denote the solution to this linear program instead of by Y-star and Z-star, let's put the-hat there. So, Y-hat-star and Z-hat-star are the optimal solution to this linear program, these points we can find. What's the relation between the objective function for the linear program compared to the ILP objective function at the optimal point? Where the value of the objective function for this LP at this optimal point, is the sum of the Z-hat-stars. How do these compare? Well any feasible point for the ILP is also a feasible point for the LP. Because the ILP constrains it to lie in this grid, the LP considers any point. So, the LP is considered more points so, since the ILP optimal point is also feasible for the LP, and the optimal solution for the LP is at least as good as the optimal solution for the ILP. So, we know that this is the objective function value, is at least this objective function value for the ILP optimal solution. So, we can find upper bound on the maximum number of satisfied clauses in F. Does that do us any good? Well not necessarily, that doesn't help us find M-star in any way. What we're going to do now, is we're going to convert this LP optimal solution into a feasible point for the ILP. It may not be an optimal solution to the ILP but it will be feasible. How do we do this? Will we have to convert this LP solution into a point on the grid. The simplest way to do that is to convert these fractional points these Y-hat-stars to integer points by rounding them to the nearest integer point, and we're going to do this in a probabilistic way. Now once we round these to integer points then we have a valid true false assignment for our set input F. And then we're going to prove that this rounded integer point is not that far, not that much worse than the original LP solution, than the fractional solution. And since this fractional solution is at least as good as the optimal integer solution, then we know that the integer solution that we found is not that far off from the optimal integer solution. So, we're going to take this LP solution around it, that'll get worse than this optimal solution, we will show that this integer solution that we find is not far off from this fractional solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916cf8f1",
   "metadata": {},
   "source": [
    "## 471 Rounding\n",
    "\n",
    "Once again, we take this LP, and we find the optimal point, and we denote the optimal point by this vector Y ^* and Z ^*. Hats correspond to LP's, so they might be fractional values. Without the hats, it corresponds to integer so our goal is to find an integer point, we'll denote it by YI and ZJ. We drop the stars because it might not be optimal any longer, but we want this integer point that we find by rounding this point. We want this integer point that we find to be close to the optimal integer point. How do we prove that this point that we find, this integer point that we find is close to the optimal integer point because we don't know at this point? Well, we show that this rounding procedure doesn't change the objective function too much. So this integer point that we find is close to this optimal fractional point, this optimal LP solution, and this is at least as good as this. Therefore, if this is close to this one, then it's also close to this point. How do we round from this fractional point to this integer point? We'll call our LP and our integer linear program had the constraint that these variables are constrained between 0 and 1. Therefore, this optimal solution also satisfies these constraints so YI*^ is between 0 and 1. Therefore, we can think of it as like a probability so we're going to round this, so we're going to set YI to be 1 or 0 with probability proportional to this. With probability YI^* is between 0 and 1, and it's a real number. With that probability, we set YI to be one. So if this is 3/4, then with probability 3/4, we set YI to be 1. And with probability 1/4, we set it to be zero. This is known as randomized rounding. Now this actually completes our algorithm. We have assignment now. We have a true false assignment for the variables in the original formula. So we've taken this fractional point, and we round it to an integer point. Notice we don't have around these Z's, we just have to round these Y's. Now, if YI = 1, then we that the variable XI to be true. If YI = 0, we set the variable to be false. So we have a true-false assignment for the X's, and it's a randomized algorithm for setting these X's. So, as we did before, we wanted to look at the expected performance of this randomized algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b7eb1",
   "metadata": {},
   "source": [
    "## 472 Expectation\n",
    "\n",
    "Once again, we want to look at the expected performance of this randomized algorithm that we just defined. Therefore, we let capital W denote the number of satisfied clauses. This is in the random assignment. The assignment produced by solving the LP and doing randomized rounding. Since the assignment is random, capital W is a random variable so we're going to look at its expectation. Now, to simplify the analysis of the expectation, we're going to do a clause by clause analysis. As before, we consider each clause, so we consider the J clause, and we denote the random variable WJ takes value 1 if this clause CJ is satisfied, and 0 if it's not satisfied. Now if we sum WJ, where J going from one to M, then for each satisfied clause we get a count of 1. So the total count is going to be the total number of satisfied clauses which equals capital W. We can analyze the expectation of capital W, the expected number of satisfied clauses in a manner similar to we did earlier in the lecture. We can use this identity to re-express expectation of W to be the expectation of this sum. And then we can apply linear narrative expectation and take the sum from inside the expectation to outside. We get that the expectation of W equals the sum over J from 1 to M of the expectation of WJ. Now, the expectation of WJ is quite simple because it's a 0-1 random variable. So the expectation of WJ in this case is just the probability that this clause CJ is satisfied. Now we're going to prove that the probability that this clause is satisfied, is at least 1 minus 1 over E times the LP value for this clause. The LP value for this clause is ZJ hat star. In some sense, this is like the probability that this LP satisfies this clause and this rounding procedure afterwards. This assignment that we end up with will satisfy this clause with probability at least 1 minus 1 over E times this original probability. We're going to put this lemma momentarily, but now let's plug that back into this computation. We can take this 1 minus W over E outside of the sum so we have 1 minus 1 over E times the sum from J going from 1 to M of Z-J had star. Now, what does this quantity? This is the value of the objective function for the linear program. What do we know about the linear program versus the integer linear program? Well, the linear program is at least as good as the integer. So this value of the objective function is going to be at least the value of the optimal for the integer linear program. This is going to be at least 1 minus 1 over E times this value for the integer linear program. For the integer linear program it's equal to the max sat so it's equal to the maximum number of clauses satisfied which we denoted by M star. And in conclusion, we can show that the expected performance of this algorithm, the number of satisfied clauses in expectation, is going to be at least the optimal number times 1 minus 1 over E. So we're going to be within 1 minus 1 over E of the optimal number. Therefore, we have a 1 minus 1 over E approximation algorithm which improves upon our one-half approximation algorithm. So we simply have to prove this lemma and then we have our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fdc27b",
   "metadata": {},
   "source": [
    "## 473 Lemma\n",
    "\n",
    "Take a look at this lemma that we're trying to prove. Look at a clause Cj and let's say Cj is of size K and let's suppose the clause is X1 or X2 and so on, up to XK. So assuming all the literals are positive form. Let's just do the analysis for this simpler case and the same approach will apply to the general clause. And in fact since we're just looking at this clause in isolation and because of the symmetry, we can always convert this clause to appear in this form. Now, what was the LP constraint for this clause? Let's look at it in terms of the optimal solution to the LP. So variable X1 has this variable in the LP Y1_hat_star. We want that if this is 0, meaning X1 is false and if that's the case for all K of these variables, then we want that this variable Zj_hat_star is 0 in this case. So if all of these literals are unsatisfying, meaning all these variables are set to false then all of these Y's are set to 0, then Zj must be 0 corresponding to this clause being unsatisfied. This is the LP constraint. Now, we're trying to analyze the probability that this clause is satisfied after the randomized rounding procedure. This is of course the same as 1 minus the probability that this clause is unsatisfying. What's the probability that this clause is unsatisfied? Well, what's the probability that all these variables are set to false? So we're going to look at the product over these K variables. The I one is set to false with probably one minus Yi_hat_star. Now, for your call dilemma, we're trying to show that the probability that this clause is satisfied is at least 1 minus 1 over E times this quantity Zj_hat_star. What we know is that the probably it satisfied equals 1 minus this product, of these Yi's. Now we know how to relate the sum of these Yi's to this Zj_star. How do we relate the product to the sum? So we want to relate this product to this sum and then we can relate it to Zj_hat_star. To relate this product to this sum, we're going to use the geometric mean, arithmetic mean, inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f0a863",
   "metadata": {},
   "source": [
    "## 474 AM GM\n",
    "\n",
    "Let's take a look at the arithmetic mean, geometric mean and inequality. Let's take a look at its general form. We have K non-negative numbers, W1 through WK. Think of these as weights. We want to look at the mean of these weights. So we can look at the arithmetic mean and the geometric mean. The arithmetic mean takes the sum and divides by the number of terms. The geometric mean takes the product and takes the K through. How do these two things relate? While the arithmetic mean is always at least the geometric mean. And this holds for any non-negative weights. How do we relate this to our problem? Well, we're going to set WI to be one minus YI star hat. And then the arithmetic mean is one over K times the sum of one minus Yi hat star And this is at least the geometric mean, which is the product over I of one minus YI hat star and then we take the K through of the right hand side. Now, alternatively, instead of taking the K through to the right hand side, we can take the Kth power of both sides and we have the following inequality. This is the one we're going to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b143bbe",
   "metadata": {},
   "source": [
    "## 475 Analysis\n",
    "\n",
    "Going back to our analysis from earlier, we're looking at the probability that this Jth clause is satisfied. This equals one minus the probability it's unsatisfied, which is the product over I, from one to K of one minus YI hat star. Now, we can apply the arithmetic mean, geometric mean and inequality. Now, we can apply the arithmetic mean, geometric mean and inequality. The arithmetic mean is one over K times the sum of I going from one to K of one minus YI hat star. And then we're raising it to the Kth power, since we dropped that one over Kth root of the geometric mean. Now, the inequality said that the geometric mean is at most the arithmetic mean. But we're doing negative of this so we get that it's at least. Now, we can simplify this a bit. We can do the sum over each of these terms separately. And the first term is a sum over I from one to K of one. That's simply K. So, that divides by this one over K. So, we get one for the first term. So, we get a one for this first one, and the second term becomes one over K times the sum over I from one to K, of YI hat star. And then we get this whole quantity raised to the Kth power still. Now we're in business, because we have this sum over these YI hat stars. Now, if you recall before, we had this constraint from the linear program. It said that if each of these variables was set to false, so each of these Ys was set to zero, then the corresponding ZJ must be zero. Now, this is actually a linear program, so we don't have this integral constrains. So, we simply know that the sum of these Ys is at least this real number Z. But, we still have this inequality, and our goal is to relate this probability that this clause is satisfied to this quantity ZJ hat star. So, we can apply this inequality, plug it in, and we can drop these Ys and get the Z. Now, this is at least. Then we're doing a negative, and then another negative. So, we get that this is at least one minus the quantity, one minus the ZJ hat star over K, all raised to the Kth power. So, we replace the sum over Ys by ZJ hat star. Now, we have to do a bit of calculus to simplify this. What we want to do is we want to move this ZJ outside of this Kth power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce330a64",
   "metadata": {},
   "source": [
    "## 476 Calculus\n",
    "\n",
    "Let alpha to note ZJ hat star. What we've shown so far is that the probability that this clause CJ satisfied is at least one minus one minus over alpha over K to the K. Now, once again, we want to get this alpha outside of this power. We want to make it a linear function of alpha. Let's look at this function of alpha. Let's define F of alpha to be this right hand side. And our claim is that F of alpha, so this right hand side here, is at least some constant times alpha. It's a constant in the sense it's independent of alpha. It does depend on K, the size of this clause. Now, what does that constant there? Well, it's going to be similar to this without the alpha. It's going to be one minus quantity one minus one over K raised to the Kth power. Once we prove that, then we can apply that here and we get that this is at least this quantity, which is this constant times alpha. We got the alpha outside of the power so it's now linear in alpha. It's linear in this Z. Now, how do we prove this claim? First off, you take the second derivative and you prove that it's negative. Therefore, this function is concave. Since F alpha is a concave function, it's going to look something like this. We're trying to relate F of alpha, this curve, to this linear function. Let's denote this linear function is beta-alpha, so this quantity here is beta. Let's look at this line beta-alpha. For large values of alpha it might not be bigger. But notice, what values of alpha are we interested in? We're interested in alphas between zero and one, so we only need this claim for values of alpha between zero and one. So, since it's a concave function it's got one peak. If we look at the end points that we're interested in, one and zero, and if we prove that this curve is above this line at these two points, zero and one, then it's also above at every intermediate point. So once we take the second derivative and prove it's concave then it suffices to check the point zero alpha, equals zero and alpha equals one. And if you plug in alpha equals zero, we get zero on both sides, and alpha equals one, it's straightforward to check that that's also true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73503342",
   "metadata": {},
   "source": [
    "## 477 Finishing Up\n",
    "\n",
    "Okay, we're almost done. Actually, we really are done. We just have to summarize. We're looking at the probability that this clause, Cj, is satisfied. This is equal to one minus the product over I, of one minus Yi^*. Then we apply the AM-GM inequality to convert this product into a sum. Then we apply the LP constraint to relate that sum over the Yi's to Zj. And then we got this inequality. We got that this is at least one minus, one minus the Zj^* over K, raise to the k-power. Now what we just proved is that this whole thing is at least, this constant which is a function of K, but independent of Z, one, minus the quantity one minus one over K to the K, times ZJ^*. Okay, now let's simplify this a little bit. If you recall, the Taylor series for E to the -X, it's one minus X, X squared over two, X-cubed over three factorial and a plus-minus, alternating series. Now, what if we want to relate E to the minus X to 1 minus X? Well, the next term is positive. So E to the minus X is going to be bigger. So one minus one over K is at most E to the minus one over K raised to the Kth power. We have 1 over E, and then take minus and then we have it, it's at least. So we have this quantity becomes one minus one over E, times Zj^*, and that's what we wanted to prove. That was the lemma we claimed. And therefore, we have a one minus one over E approximation algorithm. That completes the analysis of this LP-based algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff0dc9",
   "metadata": {},
   "source": [
    "## 478 Summary\n",
    "\n",
    "Now, let's summarize some important points about this LP-based algorithm. It's a very general approach. We can take a hard problem and oftentimes we can reduce it to integer linear programming. Integer linear programming is very general, very powerful technique. So it's very common that we can reduce these NP-hard problems to integer linear programming. Then we can relax it to a linear programming. We just drop this integrality constraint, and then we can solve it. There are polynomial time solvers, though we can often use simplex algorithm. It's often fast. Then we can take this LP solution and we can round it into some randomized way as we just did for its SAT. And this gives us a feasible point for the integer linear program and hopefully, it's a reasonable heuristic for this solution, the optimal value of this ILP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c16470",
   "metadata": {},
   "source": [
    "## 479 Comparison\n",
    "\n",
    "Now, we've seen two algorithms from Max-SAT. Now, what if we look at Max-SAT on exact K-SAT formulas? So these are formulas were all clauses have size exactly K. And let's look at the performance of these two algorithms we've seen so far as a function of K. So we have the simple algorithm, that's a simple randomized assignment. So each variable is set to true with probably a half, and false with probably a half. And we have this LP based scheme. So we write Max-SAT is an integer in linear program, we relax it to a linear programming, and then we round it. How do these two schemes compare for different values of K? Let's take a look. Let's look at the performance of these two algorithms for K=1. So it's all uni clauses, so it's quite trivial, K=2 and K=3, and in general K. If you recall the simple algorithm, the probability of clauses not satisfied is 2 to the -K. So the probability this satisfied is 1-2-K. So in general for k, it achieves of 1-2-K approximation factor, plugging in K=1, and get one-half. K=2, we get three-quarters, K=3, we get 7 A's. Now the LP, the general form, was quite complicated. We proved for general k that it achieves one minus the quantity, one minus one over k to the kth. Plugging in K=1, we get 1, which is quite good. Simple scheme was quite bad for K=1, but the LP based scheme is quite good. Now K=2, they match, both three-quarters. For K=3, we got one minus two-thirds cubed. If you plug that into a calculator, that's roughly 0.704. So for K, at least three, the simple scheme beats the LP scheme. But for small clauses, the LP scheme is at least as good or even better. The key observation is that if you look at each row, the max in each row, the best of these two schemes for every K is at least three-quarters. So, can we combine these two schemes to achieve a three-quarters approximation? Yes we can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6b570",
   "metadata": {},
   "source": [
    "## 480 Best of 2\n",
    "\n",
    "Here's our combined algorithm. Take our input formula F. We first run the simple randomized algorithm, which assigns each variable true or false independently with probably a half, and we get an assignment which satisfies, let's say M1 clauses of F. So M1 is the number of clauses satisfied by this simple algorithm, this simple randomized algorithm. We also run the LP-based scheme, and we get an assignment, we look at how many clauses are satisfied by that assignment. Let's say it satisfies M2 clauses and whichever of these two is better, we take that assignment. So we look at both of these assignments and we take the better of these two assignments, that's our algorithm. Now if we look at the expected performance of this algorithm, what we're going to achieve is the max of these two. So the performance, the expected performance of this best of two algorithm, is the expectation of the max of M1 and M2. That's what our algorithm is going to achieve. And what we can prove is that this max is at least three-fourths the optima value. Why is that? Well, that follows from the fact that each of these algorithms, the best of these two algorithms for each row on the previous slide, is at least three quarters. For every specific K, we get at least three quarters, and then we can analyze this in a clause by clause manner, so that we get at least three quarters of the optima value. You can look at the online notes to see the calculus behind this. But the punch line is that this algorithm, this combined algorithm, gives a three quarters approximation algorithm for max M, even when the formula has clauses of some small and some big. So even with formulas with varying length clauses, we achieve a three quarters approximation."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
