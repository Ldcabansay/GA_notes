{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7c57764",
   "metadata": {},
   "source": [
    "# GR4 Markov Chains and PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ecded",
   "metadata": {},
   "source": [
    "## 308 PageRank Lecture Outline\n",
    "\n",
    "In this lecture, we're going to look at the PageRank algorithm. This is an algorithm for assigning the importance of a webpage. Now, what exactly do we mean by importance? Well, this is a subjective term, but we're going to give a precise quantitative interpretation of importance. Now, the PageRank algorithm is the algorithm devised by Brin and Page which is at the heart of Google search engine. Now, the PageRank algorithm itself is fairly simple, but to fully understand the algorithm, we have to first understand some basic mathematical tools known as Markov chains. Now, I'm going to give you a quick primer on Markov chains. What exactly are Markov chains? And what are the key properties of Markov chain? Now, the PageRank algorithm is itself a Markov chain but Markov chain is come up quite often. For example, you may have heard of the term MCMC, That stands for Markov chain Monte Carlo. Also, you may have heard of simulated annealing, which is another example of a Markov chain. Now, we're not going to look at these two examples in detail, but if you understand Markov chain, the basics of Markov chains, then that'll help you understand these more sophisticated concepts if you decide to delve into them in more detail. Now, the key concept for Markov chain is the notion of a stationary distribution. So, I'm going to spend some time explaining to you what exactly does a stationary distribution mean. How do we determine what the stationary distribution are and what are the important properties of a Markov chain which connect to the notion of a stationary distribution? Finally, once we understand Markov chains and stationary distributions for Markov chains, then we will be able to fully appreciate the PageRank algorithm and the design choices in the design of the PageRank algorithm. Now, let's go ahead and dive into a Markov chain and look at a specific example of a Markov chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0c3a4",
   "metadata": {},
   "source": [
    "## 309 Example Markov Chain\n",
    "\n",
    "Now, this directed graph is an example of a Markov Chain. Now this example is meant to illustrate your state of being at various times while sitting in CS 6210. It certainly doesn't illustrate 6505 since we have this sleep here. Now we think a discretizing time. So that the time is a parameter t which goes from zero, one, two. It has integer values. You can think of the time as being like the time in seconds or the time in minutes. Now for this particular example there are four possible states at each time. You can be listening to Kishore, you can be sleeping, you can be checking your email or you can be playing this video game StarCraft. So each vertex in this directed graph corresponds to a state of the Markov Chain. So there's a Markov Chain has four possible states. Now the edges of this directed graph have weights. The weights correspond to the probability of a transition. So the weight of the edge is between say, checking email and StarCraft is the probability of changing from checking email at time t to playing Starcraft at time t plus one. So let's say I'm checking email at times zero. Then at time one, with probability point three I'll be sleeping, with probability point five I'll be playing Starcraft, and with probability point two I'll be listening to Kishore. Similarly, if I'm listening to Kishore at some time t then at time t plus one with probability point five I'll be listening to Kishore again and with probability point five I'll be checking email. So in general, a Markov Chain is defined by a directed graph and one key thing is that this directed graph might have self loops. For instance, if I'm listening to Kishore at time t then with probability point five I'm listening to Kishore at time t plus one. Now notice that the out edges out of each vertex to find the probability distribution for the next state. So if I'm checking email at time t then I'm listening to Kishore with probability point two, sleeping with probability point three, and playing Starcraft with probability point five in the next time-step. Notice that these edge weights point two plus point three plus point five have to sum up to one because this is a probability distribution for the next state. So for every state i, in this case i equals check e-mail, if I sum over the out edges, so I sum over the weights of these out edges, this is P(i,j). Then, what do these some to? They have to sum up to one because this is a probability distribution for the next state given I'm in state i at time t then I'm going to be in state j at time t plus one. Also, what are valid edge weights? Will these correspond to probabilities? Probabilities have to be between zero and one. So all the edge weights are between zero and one. You give me any directed graph with edge weights between zero and one, that defines a Markov Chain. And similarly, any Markov Chain can be viewed as a directed graph with these edge weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211bdf7f",
   "metadata": {},
   "source": [
    "## 310 General Markov Chain\n",
    "\n",
    ">> So how do I define a Markov chain in general? In general I'm going to have capital N states, and we are going to label these states by 1 through N. So in this example, capital N equals four. But in general, think of in our applications capital N is going to be huge. For instance, when we do PageRank, capital N is going to be the number of webpages on the internet. The total number of webpages on the internet. Now, this weighted directed graph is defined by its adjacency matrix. This is the adjacency matrix for this graph where we think of this is vertex, this state. Listening to Kishor is state one, this is state two, playing Starcraft is state three and sleeping is state four. So this is the adjacency matrix for this graph, the weighted adjacency matrix. Now I denote this by P, and we refer to this as a transition matrix Y, because the entry P I J corresponds to the probability of transitioning from state I to state J. So if I'm at state I at time T, the probability I'm in state J at time T plus 1 is exactly the entry P I J. I look at row I and column J. For instance, if I'm playing Starcraft at time T, the probability that I'm checking email at time T plus one is exactly 0.3. Now, the property that you noticed before about P is that each row sums up to one. The terminology for this is that P is a stochastic matrix. If the columns also sum to one, then it's called doubly stochastic. But for a Markov Chain, all I know is that the rows sum up to one. It doesn't necessarily mean that the columns have to sum up to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea371302",
   "metadata": {},
   "source": [
    "## 311 2 Step Transitions\n",
    "\n",
    "Here again, is our earlier example of a Markov chain with four states. This is a weighted graph. But for a moment let's ignore the edge ways and let's look at the unweighted version of this graph. Here's the adjacency matrix, where this unweighted version of this graph. From State 1, there are edges to itself to State 2 and that's it. So the first row looks like 1 1 0 0, and is straightforward to check the remainder of the adjacency matrix. Now, I want to look at the matrix, A square. So A times A. This is still going to be a four by four matrix. Let's simply multiply it out, and what do we get? I claim that this is the matrix A square. What do these entries mean? Well, take a look at this first entry, 1 1. It has value 2, why? Well, because there are two pass going from State 1 to State 1 of Lane 2 In particular, I can self loop and then self loop, or I can go to State 2 and then back to State 1. Take another State, four comma one. From State 4, look at the pass of Lane 2 to State 1. I can self loop and then follow this edge or I can follow this edge, and then self loop. But if I look at from four to two, what can I do? Well, there's only one path of Lane 2. I can go to State 1 and then over to State 2, and if I look at the pass of length two from 4 to 3, where there are no pass of Lane 2 that go from State 4 to State 3. There's no way to reach 3 from 4 by path of Lane 2. So this matrix A square encodes the number of pass of Lane 2. Now let's go back to the weighted version of this graph. This is the transition matrix for this Markov chain. Let's write the transition matrix once again for this Markov chain. I got half probability of staying at State 1, half probability of going from State 1 to State 2, and then zero probability of going from State 1 to State 3 or State 4. So this is the first row. This is the remainder of the transition matrix. We're going to look at this matrix, P square, which is P times P. Well let's simply multiply it out first and see what matrix we get. Well if we multiply out P times P, this is the matrix P square that we get. What are these entries mean? Well one thing we noticed first, is that we get a 0 entry here, and we also had a zero entry in the matrix A square. But why is that? Well, there's no passive Lane 2 in this graph, from State 4 to State 3. So this entry four, three is 0. These other entries there are no longer integer values, so they no longer correspond to the number of pass between this pair, I J. But they too correspond to the total weight of the pass, from I to J. Suppose we start the Markov Chain at time zero, in State 2, checking email. And I ask, what is the state at time t=2? Two time steps away. Well, one time step away is defined by the matrix P. Now let's look more precisely at the probability of going from State 2 at time zero, to State 1 at time two. So what's this probability of going from State 2 to State 1, in two time steps? Well, there are two ways to do it. I can go from 2 to 1, and then self loop, or I can go 2 to 4 and then to 1. The probability of going from State 2 to State 1 is point 2, and then from 1 to 1 self looping is point 5 probability. The other path has probability point 3 times .7. Now if you work this out, what do you get? You get point 31, which is exactly the entry two one in this matrix P square. So in this matrix P square, at the entry I J, tells us the probability of going from State I to state J in exactly two times steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235cadc3",
   "metadata": {},
   "source": [
    "## 312 k Step Transitions\n",
    "\n",
    "Now if we look at this entry (i,j) , in this matrix P, this transition matrix P, corresponding to the way to the adjacency matrix. This tells me the probability of making the transition from I to J in one step. So if I'm in state I at time T, then PIJ is the probability that I'm in State J at time T plus one. Now what we just saw is that if we look at the square of the transition matrix, so P squared, then the entry (i,j) tells me the probability of going from state I to state J in two steps. And in general for any non-negative integer K, P to the K. So the Kth power of this transition matrix P. This tells me the probability of making a transition in exactly K steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17f17e",
   "metadata": {},
   "source": [
    "## 313 Big k for 6210 Example\n",
    "\n",
    "Now let's stick with our 6210 example, and here's the transition matrix again for that 6010 example. And now I want to look at powers of P. So I want to look at P to the K for big K and then we're going to see some interesting properties about this matrix. Now once again what we saw before the square of this matrix, so P squared is this matrix. Now let's look at it for big K. So let's look at it from P the 10 and P to the 20. Now actually if you like to code up, you just code it up yourself. Take this matrix and look at powers of it for big K, or take a different matrix. Make up a matrix and look at powers of it for big K. Just make sure that it's not stochastic matrix so each row sums to one, then it corresponds to a Markov chain. So this is the first row for P to the 10th. And here's the second row. Notice that it's quite similar to the first row. Is that just the fluke? Let's look at more rows to see whether that was just a fluke or it has some important properties. Now the exact numbers in this matrix aren't important, but what's important is this interesting property that seems to be coming up. All the roads seem to be converging to the same value. Now there's still a little bit of variation look in this third column. Let's see what happens for P to the 20. While looking at the first column of P to the 20, we noticed that it seems to be converging quite nicely. They all agree on the first four significant digits. If we look at the other columns, we see that those are converging quite nicely as well. So what's our conclusion? Our conclusion is that there seems to be a row vector and all of the rows are converging to this row vector. Now let's look at this matrix see what it means. Take any particular column. Let's take column two. Now let's look at this entry one, two in this matrix P to the 20. What does that mean? Well it means if I start in stage one at time zero, what's the probability that I'm in stage two at time 20? Well that's exactly this entry. Similarly, if I start in stage two or three or four, what's the probability that I'm in stage two at time 20? Well regardless of where I start in, it seems like it's independent of where I start in. At time 20, it's going to be exactly this. And if I look at a larger time then this is going to converge even more. So there's going to be a specific probability that I'm in stage two for big time regardless of where I start at time zero. So that's the key property of this Markov chains. Regardless of where you start, it doesn't matter. If you look for big time, I'm going to converge to some value. So where I am at some big large time is independent of where I start. Let's say that again a little bit more precisely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f856856",
   "metadata": {},
   "source": [
    "## 314 Infinite Time\n",
    "\n",
    "Now, what we saw in the previous slide was we looked P raised to the power T or K for big T. We did T equals 20, but let's take T going to infinity and see what it looks like. So let's take the limit as T goes to infinity and look at P raised the power T. It turns out there's this row vector pi. Now these entries might look quite familiar. They look very similar to the row of P to the 20. Now what is P to the T going to look like as T goes to infinity? Well, each row is going to converge to pi. So every row is going to converge to this row vector pi. What does this mean? This means that no matter where you start, it doesn't matter where you start, because that's the row here, independent of where you start. If you look for big enough T, the probability that you are at state J at time T is going to be exactly defined by this row vector pi. So pi of J is going to be the probability that I'm in State J at time T, for Big T. So for our 6210 example, what does this mean? This means no matter where you start at time zero, if the class is long enough, the probability that you're sleeping at time T is exactly 0.104. And similarly the probably you playing Starcraft at time T for big T is exactly point 0.406 for big T. Regardless of where you start at time zero. This pi is referred to as a stationary distribution. You can think of is like a fixed point of the process. For this particular example, regardless of where you start, you eventually reach this stationary distribution and once you're at the stationary distribution, you're going to stay at the stationary distribution. It's going to be invariant. Now what we want to understand is, does every Markov chain have a stationary distribution? And does every Markov chain have this property that regardless of where I start I eventually reach this stationary distribution? Moreover, is there a unique stationary distribution or is there multiple stationary distributions? If you think of the analogy with fixed points are the multiple fixed points? Well, there's only one fixed point and regardless of where I start, the basin of attraction is everywhere. So regardless of where I start, I always reach that one attractive fix point. So is there one stationary distribution which I reach regardless of where I start? Or can there be multiple stationary distributions? Certainly there can be multiple, but we want to look at conditions where there are a unique stationary distribution and regardless of where I start, I always reach that stationary distribution. Then we want to look at what is this pi? What is the stationary distribution? Now this is quite important. The stationary dispersion y. For page rank, what is going to correspond to where we're going to do a random walk. A Markov chain on the web pages and then the page rank is going to correspond to the stationary distribution of that Markov chain. So all this technology is going to be useful when we're trying to understand the page rank algorithm. Now before we move on and look at details about stationary distributions, I want to look at it from another perspective. I want to look at it from a linear algebra perspective. What does a stationary distribution pi mean from a linear algebra perspective?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe0d2ab",
   "metadata": {},
   "source": [
    "## 315 Linear Algebra View\n",
    "\n",
    "Now here's a running example once again. Now let's suppose that at time zero I'm in state two and I want to know the state at time t equals one. What's the distribution for the state at time t equals one? Well, how do I get it? Why just look at this row. The second row of this transition matrix tells me the distribution for the state at time t equals 1. It's the one step transition matrix. Now what's another way to get row two. Well, I can take this vector which has a one in entry two and zeros everywhere else and I can multiply this row vector by this matrix, and then what do I get? I get row two of this matrix. So this my distribution at times zero and I multiply that by the transition matrix and I get the distribution at time t equals one. And in general at time zero I don't have to be in a fixed state. I can be in a distribution over the states. So let M be an arbitrary distribution over the end states. So what exactly does that mean. That means M is this vector and this row vector of size N and it's a probability distribution, so the sum of these entries is exactly one and all these entries are between zero and one. Now we're placing this factor by distribution, M. Then here we have P and then we get a distribution at time t equals one. So let's call this M zero to denote the distribution at times zero. And over here, we get a distribution for the time t equals one, so let's call this M one. In general, if you take M zero so the distribution at time zero and multiply by this matrix P, the transition matrix, then we get the distribution for the state at time t equals one. So we take this row vector for the distribution of time zero, multiply by one step. So we do one step of our random walk and then we get the distribution, the row vector for the state at time t equals one. Now the key property is that for a stationary distribution pi for any stationary distribution pi. So if I am in the stationary distribution at time zero or at any time t and I look at the state at time t plus one, then what is the distribution going to be? Well once I reach this distribution, I stay in it. It's the limiting distribution. It's like a fixed point of the process. So once you reach a fixed point you stay in a fixed point and that's the same for a stationary distribution. So if I'm in the stationary distribution at time t, and I do one step then I'm still in the stationary distribution pi. So pi times P equals pi. What does that mean in terms of linear algebra? Well this pi is an eigenvector with eigenvalue one. So pi is an eigenvector for this matrix P and it's an eigenvector with eigenvalue one. One turns out to be the largest eigenvalue for this matrix. So this is the principle eigenvector for this matrix. Now there can be multiple eigenvectors with eigenvalue one. We're going to look at situations where we know that there is at most one eigenvector with eigenvalue one. So there is at most one stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a82a5",
   "metadata": {},
   "source": [
    "## 316 Stationary Distribution\n",
    "\n",
    "Let's recap what we know so far about stationary distributions of a Markov Chain. So let's consider a Markov Chain defined by the transition matrix P, now if our Markov Chain is defined on end states then we consider any distribution pi on those end states, so this is a row vector of size N, so for any pi which satisfies pi times P equals pi, so pi once again is a eigenvector with eigenvalue 1 for P then such a pi is a stationary distribution. What this equation says is that, if we start in distribution pi and we do one step of our random walk defined by P then we'll stay in the stationary distribution pi, so pi' isn't variant. Once we reach it we stay in it. Now this defines a stationary distribution. Now, when is there such a pi? What Markov Chains have a stationary distribution? And if there is one, is it unique or there's multiple ones? Under what properties do we have multiple or unique stationary distributions? And for our simple 62-10 example, we noticed that no matter where you started, you always reached the stationary distribution. So in this case when there's a unique stationary distribution, regardless of where we start, do we always reach this stationary distribution? Finally, in this case where there is a unique stationary distribution and we always reach it, we could ask how fast we reach it. This is known as the mixing time of the Markov Chain. How fast the Markov Chain reaches its stationary distribution. This is one of the things I study in my research, trying to prove bounds about the mixing time of Markov Chains. We're not going to look at the mixing time here but what we are going to look at, our properties of the Markov Chain which ensure that we have a unique stationary distribution and that we always reach the stationary distribution. So regardless of where we started from, we eventually in the limit over time will reach a stationary distribution. In order to see which properties ensure a unique stationary distribution, let's look at examples where we have multiple stationary distributions. That will give us some insight into the properties needed to guarantee a unique stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c2bd9",
   "metadata": {},
   "source": [
    "## 317 Bipartite Markov Chain\n",
    "\n",
    "Now here's an example of a Markov chain whose graph corresponds to a bipartite graph. Let's look at some basic properties of this Markov chain. Suppose we start our random walk from one of these vertices on the left side, either state one, three, or five. What do we know? We know at time one we're on the right side, and actually at any odd time we're on the right side and at any even time we're on the left side. Now suppose we start on the right side, so at times zero we're at either vertex two or vertex four. Now we have the opposite situation, at odd times where on the left side, at even times we're guaranteed to be on the right side. So the punch line is that the starting state matters in this graph. Whenever we have a bipartite graph the starting state matters. Now we want a simple way to ensure that our Markov chain is not bipartite, and even more so we want to ensure that the Markov chain has no periodic structure. Instead of being bipartite, having these cycles of period two we could have cycles a period three. So in general, if we want to ensure that the graph has no periodic structure, that is aperiodic. Well what's an easy way to ensure that? An easy way is to have a self loop on each vertex. So with some probability we stay where we are. This can be a very small, miniscule probability. So let's say with probability point 01 we stay where we are, and then we can rescale the other probabilities so that they sum up to one. In terms of their transition matrix, this means that our diagonals, the self loops are out diagonal entries in this transition matrix. We want all of these diagonal entries to be strictly greater than zero. So we want the diagonal entries to be positive. If they're positive, that destroys any periodic structure. It can't be bipartite or any periodic structure. So for every state i, we'll make sure that the probability of going from state i and staying in state i is strictly greater than zero. Okay that gets around this pitfall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13756a3f",
   "metadata": {},
   "source": [
    "## 318 Multiple SCCs\n",
    "\n",
    "Now another pitfall that can happen is if our underlying graph has multiple strongly connected components, for example in this graph, there are three strongly connected components. Now look, if we start at one of these three vertices we're only going to reach these three vertices. If we start at one of these two vertices, we only reach these two vertices. So the starting state definitely matters. What we would like is that the graph has one strongly connected component. The terminology for this is that P is irreducible. If the graph has one strongly connected component, every pair of vertices are strongly connected with each other, then the transition matrix is irreducible. Now what's an easy way to assure that the graph is one strongly connected component, while connect up every pair of vertices. So make it the complete graph. For all pairs of states i and j, we make the entry p (ij) be strictly greater than zero. So there's a positive probability of going between every pair of vertices. The matrix P is all positive, and therefore the graph is fully connected, so it's one strongly connected component. This is the easiest way to ensure that the graph is one strongly connected component and therefore irreducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416d691",
   "metadata": {},
   "source": [
    "## 319 Ergodic MC\n",
    "\n",
    "These are the two key properties of a Markov chain, that is aperiodic and irreducible. Recall that aperiodic means that the underlying graph is not bipartite. How do we get around it? We add self-loops, and that ensures that the graph is not bipartite and has no aperiodic structure. Irreducible means that it has one strongly connected component. How do we ensure that? By making sure that the graph is fully connected. All pairs of states can get between each other in one step, there's an edge between every pair of vertices. Now, a Markov chain which is a periodic and irreducible is called the Ergodic, and that's the key property. In Ergodic Markov chain has nice properties. A Markov chain with these two properties has a stationary distribution pi and is unique. There is exactly one stationary distribution. Moreover, we have the following nice property for the raised to the power T. For some big enough T then the matrix looks like pi, pi, pi, every row is pi. What does that mean? That means regardless of where we start. So, we starting at one of these rows, and then we're doing T steps of our random walk. We're going to reach this distribution pi. So no matter the starting state, we eventually reach the unique stationary distribution pi. This was the same scenario that happened for our simple example on four states for the 6210 example. So in other words, we always reach pi no matter where we start. How is page rank going to be defined? Well, page rank is going to be defined by looking at a Markov chain on the web graph, so, we're going to do a random walk on the web graph, the sites, the vertices are going to be web pages, the edges are going to correspond to hyperlinks. Now, we're going to have some technicalities to ensure that the underlying Markov chain is aperiodic and irreducible. How are we going to solve them? Exactly as we mentioned earlier. We're going to add self loops and we're going at it to be fully connected and then the page rank is going to be defined as the stationary distribution of this Markov chain. Since it's Ergodic, there's a unique stationary distribution and no matter where we start as random walk, we'll always reach this unique stationary distribution. So, it's well-defined, the stationary distribution will correspond to the page rank. So, our measure of the importance of a web page will be related to the probability of ending at that web page. We start a random walk from any state, we run the random walk for many steps. What's the probability we end at a particular web page J? That corresponds to the page rank, the importance of the web page J."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303d832",
   "metadata": {},
   "source": [
    "## 320 What is Pi\n",
    "\n",
    "Now what is this Stationery Distribution PI? Is there a nice formula for this PI? In general no but in some cases yes, there is. A simple case is when P is symmetric, what does this mean? That means the entry p_i_j is equal to the entry p_j_i. So if I look at row I and column J, the probability of going from I to J, is the same as the probability of going from J to I, if that's the case then PI is the uniform distribution. PI of I is one over N for all I. Now that's the simplest case, a generalization is known as reversibility. It's kind of a weighted version of symmetry, in which case PI is not necessarily uniform but we can still figure out the PI easily. But what reversibility requires is that, if there's an edge from I to J then there has to be edge from J to I. Now symmetry says that these probabilities are the same. We don't necessarily need them to be the same but we need that if there's an edge from I to J, then there's an edge from J to I and vice versa. Now this is the case, then we might be able to figure out what PI is, we might have a nice formula for what PI is. If this is not the case, so the chain is not reversible. So for instance there might be an edge from I to J but there is no edge from J to I. Then, in general we have no idea what the stationary distribution, there is no way to figure it out with a close formula. Now we can try to look at P to some high power and figure out what the stationary distribution is but we're not going to get some nice formulas such as this, for the stationary distribution. That completes our description of the introduction to Markov chains. Now we can dive into the details of the page rank algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4ad8a",
   "metadata": {},
   "source": [
    "## 321 PageRank\n",
    "\n",
    "Now we're going to talk about the Page Rank algorithm. This is the algorithm invented by Brin and Page. It was published in 1998. We're going to start by forgetting about Markov chains. You can understand the basic idea of the page rank algorithm without even knowing what a Markov chain is. Page Rank is a natural simple algorithm for determining the importance of web pages. Now importance is a subjective term and it's something that we're going to have to quantify ourselves. Now part of the appeal of Page Rank is how they define importance has an interesting interpretation in terms of Markov chains. Now we're going to get to that at the end of the lecture, for now we'll forget about Markov chains and we'll just look at the simple idea for defining the Page Rank. To understand the appeal of Page Rank, let's go back and think about how search engines worked in the mid-90s. Well the search engines would maintain a database of web pages and then given a query term Q, what would they do? They would do a grep for Q. So they would search for all web pages containing that query term Q. Now it's easy to spam or trick those search engines so that many of the common query terms are embedded in your web page. That's another issue, let's ignore that for now. The bigger issue is that you have many web pages that contain this query term and now how do you present it to the user? How do you sort the web pages? You want to put the most relevant web page up front so there are many web pages let's say containing this query term Q. How do you sort them? Well that's where the importance of the web page comes in. We're going to put the most important web pages containing this query term at the beginning of our list. For example, if you search for Markov chains and CNN lets say, happens to have an article about Markov chains, I'm not sure why that might be the case but let's just say, CNN has an article about Markov chains and I have on my web page I have several lecture notes about Markov chains. Now when we do this search, this grep, both web pages, mine and CNN are going to contain the query term for Markov chains. Which one should be presented first? Well, presumably the user is more likely to be interested in the CNN article about Markov chains rather than my lecture notes about Markov chains. So let's dive into the algorithm description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f297542",
   "metadata": {},
   "source": [
    "## 322 Webgraph\n",
    "\n",
    "We're going to look at the so called Web graph. The graph on web pages. What exactly is our graph? Well the vertices of our graph are going to be web pages. Now this the humongous graph, because we have a vertex for every web page and the edges of our graphs are the hyperlinks. Now these are directed edges. A web page X might have a hyperlink to a web page Y or Y doesn't necessarily have a hyperlink back to X. So it's important we think of these edges as directed edges. Now let's introduce some notation, for a web page. X, let's define pi of X to be the rank of this page. Now the rank is our measure of the importance of the web page. Now rank or importance are subjective terms. We need to define pi of X, the rank in a sensible way. Now of course sensible is also a subjective term, but in any case we're going to define pi of X so that it has a very nice natural mathematical interpretation. Now if you just watched the bit about markup change, you might think. Why is he using pi once again? Well it's not a coincidence, before we used pi to correspond to the stationary distribution and that's going to come out later, but for now once again we're not going to talk about Markov chains. Now it would be useful to have a little bit on notation. So let's consider a page X is the vertex in our graph, so it's denoted here. Now there are a bunch of hyperlinks at the web page X. Those are directed edges out of X. We want some notation for this set of neighbors which have edges from X to them. So we're going to find this set out of X. So for a web page X, out of X are the out neighbors of X. These are the web pages which have a hyperlink from X to them. So these are the web pages Y, where there's a hyperlink from X to Y, there is a directed edge from X to Y. So out of X are the set of all Ys such that there's a link from X to Y. Similarly, we're going to have to look at the set of web pages or vertices which have a link to X. So we'll let In of X denote the set of in neighbors of X. This is a set of web pages W, which have a hyperlink from W to X. So there's a direct edge from W to X. So just remember, out of X are the out neighbors of X and in of X are the in neighbors of X. This is the only totation that we'll need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08865b30",
   "metadata": {},
   "source": [
    "## 323 First Idea\n",
    "\n",
    "So imagine you're studying for your PhD, and you're trying to come up with some measure for the importance of web pages. So you might think of the analogy with academic papers. What's the importance of an academic paper? How do you measure the importance of an academic paper? Well one way that we still use, is using citation counts. How many other papers cite your paper? What does that mean in terms of web pages? Well might be the number of links, hyperlinks to your web page. So our first idea for defining the rank of a page x, is to define pi of x to be the number of links to the page x, number of hyperlinks to the web page x. In terms of the graph, this means we're going to look at the number of in neighbors to x. How many edges come into x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4a01b",
   "metadata": {},
   "source": [
    "## 324 Problem 1\n",
    "\n",
    "So here was our first idea. We set the rank of a page X to be the number of links into the page X. So pi of X is the in degree of x. Now what's the obvious problems with this? Well, here's a simple example. Let's suppose that Georgia Tech has a webpage which lists all the faculty, okay that's probably true, and the faculty list is probably like a thousand long. One of those is going to link to my webpage. So one out of a thousand of these links links to my webpage. Now let's suppose that Georgia Tech's front page or the COCs front page has only five links on it and one of those happens to be the kishore's webpage. Now in the current measure of the rank of a page X, I get a plus one for this link and Kishore gets a plus one for this link. So both of those links count the same for us. Now that doesn't seem so fair. This is one out of a thousand, this is one out of five. So how do we get around it. Well, the obvious way is to scale it by the number of links. Now the natural solution to this problem is if George Tech's webpage has a thousand links and one of them goes to my webpage then I get one out of a thousand and if Georgia Tech's front page has five links and one of those is to Kishore, then Kishore gets one-fifth of a citation. And in general if a page Y has these many outgoing links, then each webpage is going to get one over the number of outgoing links of a citation. So Georgia Tech's faculty list webpage has a thousand links let's say, so I'm going to get one over thousand of a citation. This webpage has five links, one to kishore's, so kishore is going to get one-fifth of a citation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5819e7f",
   "metadata": {},
   "source": [
    "## 325 Second Idea\n",
    "\n",
    "So we've got a web page X. We're going to look at the In neighbors. So these are the web pages Y which have a link to X. So we're going to sum over these. The old scheme just counted the number of these In neighbors. So the old scheme you can view it as a sum over the in neighbors and we get plus one for each in neighbor. In the new scheme, we want to scale it by number of outgoing links from each of these web pages. So if Y1 has a lot of outgoing links, let's say it has 1000, then this link is going to give us one over 1000. And if this web page has only one link to X, then this one gives us one. And in general, from a web page Y, we're going to get one over the number of out links from Y. And we're going to sum this up over the Ys which have a link to X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366fcb8",
   "metadata": {},
   "source": [
    "## 326 Problem 2\n",
    "\n",
    "So here's our current proposed solution. So instead of setting the rank of the page X to be the number of in neighbors, the number of links into X, we've scaled each of those links by the number of out neighbors from that page y. Now there are some obvious problems with this let's look at one example. Let's suppose that my kids made a web page and it has only one link on it, to my web page. So under this count, I get one citation for it and now let's look at CNN web page. They probably have many links on their web page but perhaps they have an article about Kishore's awesome new research and they have a link to Kishore's web page. So if they have 100 links, Kishore is going to get one over a hundredth of a citation here whereas I'm going to get one citation. Now this doesn't seem very fair because CNN's web page is very important. So we should scale this citation from CNN based on the importance of CNN's web page. So instead of a web page having one citation to give out the web pages should have PI it's rank to give out. So CNN should have Pi of CNN its rank, that's how many citations it has to give out or that's the worth of its citations to give out. Now we can scale each of those citations as we did before based on how many links CNN has. So CNN has 1000 links in each of these web pages which has a link from it, is going to get one over thousands of CNN's worth. So CNN total citation value is Pi over CNN, that's its rank and then each web page that has a link from it, is going to get one over thousands of that worth, the recommendation value. So a citation from a more important web page, has more value because it's going to be proportional to the value the rank of the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2357c2",
   "metadata": {},
   "source": [
    "## 327 Rank Definition\n",
    "\n",
    "So, let's go ahead and formalize this idea. So, a citation from web page y, it's value is going to be proportional to the importance the rank of web page y. So, we're going to scale this quantity by pi of y. Formally, the rank of the web page x. We're going to get it by looking at this sum over all web pages y, which have a link to x. The total value of the citations from y is pi of y. And y has this many outgoing links. This is the number of out neighbors. So, this link from y to x has value pi of y divided by the number of out neighbors of y. And this is going to be the definition of the page rank of web page x. Well, now to be precise, this is not exactly the definition of the page rank of a web page x. There's a technical glitch that we'll notice by looking at this from the perspective of Markov chains. Before we dive back into Markov chains, let's look closely at this proposed definition of the rank. It's a recursive definition. So, first off, is it well defined? Is there a pi satisfying it for every vertex x? This question of whether there exists a pi satisfying it, corresponds to whether there exists a stationary distribution for the corresponding Markov chain. Now, if there does exist such a pi, is there any unique such pi? Or are there multiple pis? This corresponds to the question of whether there is a unique stationary distribution or multiple stationary distributions. We'll address both of these questions using our intuition from Markov chains. First, we'll derive this definition using Markov chains and then we'll see how to ensure that there is a unique stationary distribution. So, let's dive back into Markov chains now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e790b6",
   "metadata": {},
   "source": [
    "## 328 Random Walk\n",
    "\n",
    "Let's look at this problem from a completely different perspective. So we have the web graph G. The vertices are web pages and the edges are the hyperlinks. These are directed edges. Now let's do a random walk on this graph. What exactly does that mean? It's just like you're surfing the web. So you write a web page, you're going to follow a random hyperlink from that web page, go to the next web page, look at it for a second, hit a random hyperlink and so on. So we started at some web page say we're currently at web page X. Then let's choose a random hyperlink, so uniformly at random from all hyperlinks on this web page X and then we follow that hyperlink and then we repeat the procedure from the new web page. This is a random walk on the directed graph. From a vertex, we're choosing a random outgoing edge. So this is the Markov chain. What's the transition matrix for this Markov chain? Well for Web page Y has a hyperlink to a web page X, so there's a direct edge from Y to X in this web graph. Then the weight of this edge or in the transition matrix, this entry YX is one over the number of out neighbors from Y. The probability of following this particular link when we're at web page Y is exactly one over the number of links at web page Y. So if Y has a thousand links and the probability of following this particular link is one over X. And if web page Y doesn't have a link to web page X, so there's no edge and this transition matrix is zero at that entry, so this defines the transition matrix for this Markov chain. Now this is the Markov chain. So it has a stationary distribution. What does this stationary distribution look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b0f33",
   "metadata": {},
   "source": [
    "## 329 Stationary Distribution\n",
    "\n",
    "So recall what is a stationary distribution? A stationary distribution is any vector, any distribution pi which satisfies the following identity, pi times p, the transition matrix, equals pi. So pi is an eigenvector of P with eigenvalue 1 or in other words, if I'm in distribution pi and I do one step of the random walk to find by P, then I'm still in distribution pi. So pi is an invariant distribution. Once I reach it I stay in it. Let's expand this out to see what this means a little bit more precisely. I have pi here. If there are a million web pages then pi is a row vector of size a million. Now I have P here. Now if there are a million web pages then P is of size a million by a million. Now let me just flip this. So let us look at pi equals pi times p. So I just flip the left and right hand side and here's the right hand side, pi equals pi times P. Let's look at an entry X here. So pi of X, this X entry here.If there are a million web pages and let's say X is maybe the 900th entry. How do I get this entry? Well, this X is going to define the column I looked at over here. So this is the 900th entry here and it's going to be the 900th column here. And then what I do, I take this column and then multiply by this row. So Y is going to vary over this row and then Y is going to vary over the rows of this matrix. I'm going to multiply the first entries together plus the second entries multiplied together and so on. So I'm going to sum over Y, the number of Ys is the number of web pages. So Y is varying over all vertices in the graph, all web pages in the graph and I take the Ys entry over here which is pi of Y and I multiply by the Ys entry in this column which is the P, Y over X. So I do the dot product of this row vector with this column vector and that gives me the X entry in pi. Now let's look at this term. What do we know about this term? What do we say that the transition matrix was for this random walk and the last line? Well if there's an edge from Y to X and it's one over the number of outgoing edges out of Y and there's no edge from Y to X then this transition matrix is zero. There's no probability of this random walk going from Y to X in one step. So, for any Y which does not have an edge to X, then this term is zero. So we can drop it. So we only have to look at Ys which have an edge to X. In other words we only have to look at Ys which are in the in neighbor set of X. So we can simplify this sum over all Ys to only Ys which are in the in neighbors. So now we only consider those Ys which have an edge from Y to X and we get pi of Y again and now we can replace this term P, Y of X by this quantity. So we're going to divide pi of Y by the number of our neighbors of Y because we know that if there's an edge from Y to X then P, Y, X is exactly this. So we can replace P, Y, X by this. So we have that pi of X equals this quantity. So what have we shown? We've shown that the stationary distribution of this random walk on the web graph. If we do this simple random walk on the web graph, it's stationary distribution satisfies the following identity. This is what we just saw because pi equals pi times P and we expanded our pi times P and we got this slightly simpler expression. Now, where have we seen this before? Well, what we saw before when we ignored Markov chains and we defined the rank of a web page in terms of citation count intuition. We got this definition. And look, they're identical. This definition and this definition are identical. So these two views are identical. This intuition from citation counts and this intuition from random walks, we get equivalent definitions. Now this random walk interpretation is very appealing. Think about it. So you started any web page, you run the random walk. So you just do random surfing. Now what's the chance you edit a page X. Well, an important web page like CNN or Google, I mean, there's probably a pretty good chance we're going to end up at that web page when we do a random surfing. Whereas somebody is like my web page well there's probably a very small chance that we're going to end up there. So the stationary distribution of this random walk is a very natural nice appealing measure of the importance of a web page. So, this is what we wanted to find the page rank of the web page to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643f277",
   "metadata": {},
   "source": [
    "## 330 Problems\n",
    "\n",
    "Now here's what we just saw. We saw that if we did the random walk on the web graph, then a stationary distribution satisfies this identity. And this is what we'd like to define the PageRank for this web page to be. First off, is this well defined? Think back to our discussion about Markov chains and about stationary distributions of Markov chains. What are some of the key issues that can arise when we talk about stationary distributions of Markov chains? Was there a unique stationary distribution or are there multiple stationary distributions? Well, if there are multiple stationary distributions and it's not clear which one we are referring to. Also, we want that no matter where we start this random walk, we're always going to reach the stationary distribution. For instance, what if G has many strongly connected components? I mean, this is definitely the case. The web graph is going to have many strongly connected components. So, there might be one strongly connected component, very small strongly connected component containing my web page. And, perhaps, if you start the random walk in that strongly connected component, you might have a high probability of ending at my particular web page. Now, does that mean that I should have a high PageRank? Probably not. Now, there might be another strongly connected component, a very giant component and this giant component probably contains Google's web page. And now, if you start the random walk in that giant component, then you probably have a good probability of ending at Google's web page. So Google's web page should have a high PageRank because in that component it has a high rank. So how can we ensure that the Markov chain has a unique stationary distribution? We want to ensure that there's only one strongly connected component and also we want to get rid of any periodicity. There might be some parts of the graph which are bipartite. The easy way we discussed for making sure that there's a unique stationary distribution is to make the graph fully connected. So we're going to add edges, maybe with very small probability, but there will be edges between every pair of vertices. So, from any web page, we'll have some positive probability of going to any other particular web page. So, suppose your web browser had a random button. If you hit this random button, then it's going to take you to a random web page. The web page will be chosen uniformly at random from all web pages. So if there are a million web pages, you have probably one over a million of ending at any particular web page. Now, let's suppose you're surfing the web. What's the random walk going to look like? So, with some probability you're going to follow a random outgoing link from the current web page and with some probability you're going to hit the random button. And that's going to take you to a random web page in the whole graph. So, that random button is going to make the graph fully connected. Let's formalize this random walk that we're talking about here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405532b",
   "metadata": {},
   "source": [
    "## 331 Random Surfer\n",
    "\n",
    "So let's formalize this notion of doing a random walk on the web graph, where occasionally we hit the random button. And by hitting the random button, we're going to go to a random web page. So we're going to have an additional parameter Alpha and this Alpha is going to be the probability that we hit the random button. Actually, to be consistent with the actual PageRank definition, Alphas can be a complement of that event. So with probability one minus Alpha, we're going to hit the random button and go to a random web page in the graph. And with probably Alpha, we're going to follow a random edge out of the current web page. So this parameter Alpha is called the damping parameter. It's strictly greater than zero and it's at most one. Why is it called the damping parameter? Because what we're doing is we're scaling down this original webgraph by a factor Alpha. So we're scaling down the webgraph by a factor Alpha and then we're adding in a complete graph of weight one minus Alpha. So now let's look at our random walk. Let's say we're currently at a web page Y. Then with probability Alpha, we're going to follow a random outgoing link from Y. So if Alpha equals one, this is exactly the same as our original random walk. So we're not using this random button at all, because one minus Alpha is zero. But when Alpha is strictly less than one then we're going to use the random button sometimes. So with probability one minus Alpha, we're going to go to a random page. This destination page is chosen uniformly at random from all web pages. So this is our random surfer model. So you give me a parameter Alpha and then my random walk looks like the following. I'm currently at a web page Y, with probability Alpha. I'll look at all the outgoing links from Y and I'll choose one of those uniformly at random. And with probability one minus Alpha, I'm going to go to a random web page uniformly at random from all web pages in the graph. So I'm at this web page Y, I flip a coin or I choose a random number uniformly at random between zero and one. If this random number is at most Alpha, then I choose a random outgoing link. If this random number is strictly greater than Alpha , then I go to a random web page uniformly at random from all web pages. Now what Alpha should we choose? Well according to Wikipedia, Google apparently chooses an Alpha which is at roughly 0.85."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fff2f7",
   "metadata": {},
   "source": [
    "## 332 Transition Matrix\n",
    "\n",
    "This random walk that we just defined is a Markov chain. Let's look at the precise definition of the transition matrix for this Markov chain. Let's let capital N denote the number of web pages. So this is the number of vertices in our directed graph. This is going to be humongous. Now, we have this transition matrix of size N by N. Now, there are two cases, either Y to X is an edge in the original graph or it's not. So let's look at these two cases separately. Either Y to X is an edge in the original graph there's a hyperlink from Y to X or there's no hyperlink from Y to X. If there's no hyperlink from Y to X then the only way to go from Y to X is to use the random button. What's the chance of that? Well, there's probability one minus Alpha that we hit the random button. Now when we hit the random button, we go to a random web page. So the probability of going to a particular web page is one over the number of web pages. So it's one over capital N. So the chance of going from Y to X using the random button is one over capital N. So in this case, the probability of going from Y to X if there's no hyperlink from Y to X then it's one minus Alpha for hitting the random button. And then one over N is the probability of going to this particular web page X. Now, if there is a hyperlink from Y to X, well we can still get from Y to X using the random button, probability of that is the same. But we can also get from Y to X using the hyperlink. There's probability Alpha of following a random hyperlink. And given where in this case the probability of following a particular hyperlink is one over the number of hyperlinks. The number of hyperlinks at Y is the number of out neighbors of Y. Now, this transition matrix corresponds to a fully connected graph. Every pair of vertices has an edge. Now, the probability the weight of a particular edge might be very small but still there's an edge between every pair of vertices. So in our terminology from Markov chains this corresponds to an ergodic Markov chain which implies that there is a unique stationary distribution Pi. And regardless of where you start the random walk you always reach this stationary distribution in the limit over time. So the definition of this Pi is independent of the starting state of the random walk. So this Pi is well defined. So we can set this Pi a stationary probability of web page X to be the PageRank, the importance of web page X."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98260966",
   "metadata": {},
   "source": [
    "## 333 Sink Nodes\n",
    "\n",
    "Let's consider the random surfer model we just defined. There's a problem in the current definition in particular suppose that a page x or web page x has no outgoing links. In a random surfer model with probability one minus alpha, we go to a random web page from the entire graph. And with probability alpha, we follow a random link from this current page x. Now what happens if this page x is a sink node and has no outgoing links? What do we do in this case with probability alpha? Currently the model is not well-defined because of this case. And there are several alternatives that we can consider which will make it well-defined. The simplest option is just to self-loop. What exactly do we mean? We mean that if there's no outgoing links, if this page acts as a sink node, then with probability alpha, we just stay at the page x. We had a link from x to itself. The downside of this approach is that we're adding an incoming link into x. So this is going to artificially boost the page rank of page x. Another option is to simply remove these sink nodes. Now once we remove some of these sink nodes, then there will be new sink nodes that might be created. So we have to recursively apply this approach. We have to keep removing sink nodes recursively until there are no sink nodes remaining in the graph. The problem with this approach is that we shrunk the graph, so there are some nodes which will not get a page rank. One more natural approach I want to consider is that in this case of a sink node x, with probability alpha, we'll go to a random web page chosen uniformly at random from the entire graph. In other words, we're going to set alpha equal zero for just these sink nodes. So, for a sink node with probability one, we'll choose a random web page from the entire graph and go to that random web page. This is a quite natural approach and this is apparently what page rank actually does according to Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06510a97",
   "metadata": {},
   "source": [
    "## 334 Ergodic\n",
    "\n",
    "Let's look again at why the random surfer model is ergodic. We have our original web graph, capital G. Now, what happens in the random surfer model? Well, with probably alpha, we follow a random outgoing link in G. And with probably one minus alpha, we go to a random web page in the entire graph. This corresponds to adding in the complete graph, where capital N is the number of vertices in this original graph. So, this defines our new graph, G-prime. Now, suppose that alpha is strictly less than one. That means, with some positive probability, we use these edges from the complete graph. So, if we consider any pair of states, i and j. Let's look at the probability of going from state i to state j in one step. Well, if alpha is strictly less than one, then there's some chance, some probability, of going from state i to state j, using this last type of transition. That means, in the transition matrix, the entry i j in the matrix P is positive and all the entries in this matrix are positive. There is no zero entries in this matrix. It's a fully connected transition matrix. Therefore, if alpha is strictly less than one, then this random surfer model is ergodic. Now, what happens if alpha equals one, then we're not using this random button here? So, we're just using the original graph and there's no reason why the original graph is going to be ergodic. The original graph that we're interested in is this graph, G. Instead, we're looking at this graph, G-prime. For the case alpha less than one, we need this condition that alpha is less than one in order for it to be ergodic. But how does this new graph, G-prime, compare to this original graph, G? In particular, how does the principle eigenvector for this graph G-prime, this PageRank vector for G-prime, compare to the properties of the original graph G? While this is a somewhat vague, very triggering, but a very vague question. i don't know how to address it. But what we can look at is, what is the effect of varying alpha? How does the PageRank vector change as we vary alpha? Well, if alpha is large, if it's close to one, then this graph G-prime is close to the original graph, G. So we hope that the properties of G-prime are close to the properties of G. As alpha gets smaller, then this complete graph is becoming bigger and we're becoming further away from the original graph, G. But there's a trade-off. As alpha decreases, our convergence rate to the PageRank vector to the principle eigenvector is going to go faster. We're going to converge faster to this principle eigenvector, because of this complete graph, as this becomes bigger, yet the mix faster. Now, according to Wikipedia, Google uses alpha as 0.85. This presents a reasonable trade-off between these two scenarios. But an interesting question is, how does the PageRank vector change as we vary alpha? But if we look at alpha big like 0.99 or 0.95, compared to alpha is 0.85 or 0.75. How does a PageRank vector change? For example, if you look at the top sites, those sites with the largest PageRank vectors, how does a set of top sites change with alpha? And does their ordering change with alpha? So, if you implement the PageRank algorithm and you take a large dataset, then you can look at what is the effect of varying alpha on the ordering of the sites, according to PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c0a4b",
   "metadata": {},
   "source": [
    "## 335 Finding Pi\n",
    "\n",
    "Now, how do I find this vector pi? How do I find the stationary distribution of this Markov chain? If I want to use pi as the measure of the importance of a web page, I have to find pi, or find something which is a close approximation to it. Now to find pi, what do I do? Well, I start at some initial distribution. I can take any initial distribution. Let's define that initial distribution by a row vector Mo and then I'm going to run the Markov chain, the random walk for T-stat for Big T. Computationally, that means I take Mo multiply it by P raised to the power t. Now how big of a t do we need to use? It turns out for the random walk that we're considering, just a very small t suffices. Why is that the case? Well for this particular Markov chain we have with probability one minus Alpha, we choose a random web page. Those links corresponding to the random button allow the Markov chain to mix rapidly. In practice, to check whether you did a big enough t what do you do? Well you empirically check whether this thing seemed to converge or not. Now what do you use for this initial vector? Well, if you're running this on all the web pages, well that's a humongous quantity. So you want to use a reasonable approximation to the real pi as your starting state. Well, if you're updating the webgraph every week let's say, then I would use last week's pi as my initial distribution and then I would run the random walk for a small number of steps hopefully and then that would give me my approximation for the new pi so I use my last week's pi as my initial distribution. Now one important thing to consider, this matrix is huge. Capital N, the number of web pages, is humongous. So order N square is too big. So how long does it take me to compute this vector times this matrix? Well the naive way it's going to take me order N square time that's too long. For the Ns we're considering, there's no way you can run for order N square time, you won't even have order N square space. You need to do it in order m time, m is the number of edges in the original web graph. Now of course some web pages might have many hyperlinks out of it but typically there's going to be a constant number of hyperlinks out of each web page. So m is probably going to be on the order of N and if you think about the transition matrix that we use for PageRank, then one can implement this in order m time with just a little bit of thought and this will typically be order n as opposed to order N square. So linear time is more reasonable that's something we can implement for large N. Well that completes the description of the PageRank algorithm."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
